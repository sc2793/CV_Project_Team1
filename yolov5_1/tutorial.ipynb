{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.9.4 64-bit"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8815626359d84416a2f44a95500580a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3b85609c4ce94a74823f2cfe141ce68e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_876609753c2946248890344722963d44","IPY_MODEL_8abfdd8778e44b7ca0d29881cb1ada05"]},"model_module_version":"1.5.0"},"3b85609c4ce94a74823f2cfe141ce68e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"876609753c2946248890344722963d44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_78c6c3d97c484916b8ee167c63556800","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":819257867,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":819257867,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9dd0f182db5d45378ceafb855e486eb8"},"model_module_version":"1.5.0"},"8abfdd8778e44b7ca0d29881cb1ada05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a3dab28b45c247089a3d1b8b09f327de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 781M/781M [08:43&lt;00:00, 1.56MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32451332b7a94ba9aacddeaa6ac94d50"},"model_module_version":"1.5.0"},"78c6c3d97c484916b8ee167c63556800":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"9dd0f182db5d45378ceafb855e486eb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"a3dab28b45c247089a3d1b8b09f327de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"32451332b7a94ba9aacddeaa6ac94d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"0fffa335322b41658508e06aed0acbf0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a354c6f80ce347e5a3ef64af87c0eccb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_85823e71fea54c39bd11e2e972348836","IPY_MODEL_fb11acd663fa4e71b041d67310d045fd"]},"model_module_version":"1.5.0"},"a354c6f80ce347e5a3ef64af87c0eccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"85823e71fea54c39bd11e2e972348836":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8a919053b780449aae5523658ad611fa","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":22091032,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":22091032,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5bae9393a58b44f7b69fb04816f94f6f"},"model_module_version":"1.5.0"},"fb11acd663fa4e71b041d67310d045fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d26c6d16c7f24030ab2da5285bf198ee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 21.1M/21.1M [00:02&lt;00:00, 9.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f7767886b2364c8d9efdc79e175ad8eb"},"model_module_version":"1.5.0"},"8a919053b780449aae5523658ad611fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"5bae9393a58b44f7b69fb04816f94f6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"d26c6d16c7f24030ab2da5285bf198ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"f7767886b2364c8d9efdc79e175ad8eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}},"language_info":{"name":"python","version":"3.9.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":["<img src=\"https://user-images.githubusercontent.com/26833433/98702494-b71c4e80-237a-11eb-87ed-17fcd6b3f066.jpg\">\n","\n","This is the **official YOLOv5 ðŸš€ notebook** authored by **Ultralytics**, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n","For more information please visit https://github.com/ultralytics/yolov5 and https://www.ultralytics.com. Thank you!"],"metadata":{"id":"HvhYZrIZCEyo"}},{"cell_type":"markdown","source":["# Setup\n","\n","Clone repo, install dependencies and check PyTorch and GPU."],"metadata":{"id":"7mGmQbAO5pQb"}},{"cell_type":"code","execution_count":null,"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.8.1+cu101 (Tesla V100-SXM2-16GB)\n"]}],"metadata":{"id":"wbvMlHd_QwMG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b022435-4197-41fc-abea-81f86ce857d0"}},{"cell_type":"code","execution_count":2,"source":["%pip install -qr requirements.txt  # install dependencies\n","import torch\n","fromp IPython.display import Image, clear_output  # to display images\n","\n","clear_output()\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 2.0.0 (CPU)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# 1. Inference\n","\n","`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n","\n","<img src=\"https://user-images.githubusercontent.com/26833433/114307955-5c7e4e80-9ae2-11eb-9f50-a90e39bee53f.png\" width=\"900\"> "],"metadata":{"id":"4JnkELT0cIJg"}},{"cell_type":"code","execution_count":3,"source":["!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\n","Image(filename='runs/detect/exp/zidane.jpg', width=600)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"detect.py\", line 5, in <module>\n","    import cv2\n","ModuleNotFoundError: No module named 'cv2'\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'runs/detect/exp/zidane.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/detect/exp/zidane.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/detect/exp/zidane.jpg'"]}],"metadata":{"id":"zR9ZbuQCH7FX","colab":{"base_uri":"https://localhost:8080/","height":534},"outputId":"c9a308f7-2216-4805-8003-eca8dd0dc30d"}},{"cell_type":"markdown","source":["# 2. Test\n","Test a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."],"metadata":{"id":"0eq1SMWl6Sfn"}},{"cell_type":"markdown","source":["## COCO val2017\n","Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."],"metadata":{"id":"eyTZYGgRjnMc"}},{"cell_type":"code","execution_count":null,"source":["# Download COCO val2017\n","torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n","!unzip -q tmp.zip -d ../ && rm tmp.zip"],"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(FloatProgress(value=0.0, max=819257867.0), HTML(value='')))"],"application/vnd.jupyter.widget-view+json":{"model_id":"8815626359d84416a2f44a95500580a4","version_minor":0,"version_major":2}},"metadata":{"tags":[]}},{"output_type":"stream","name":"stdout","text":["\n"]}],"metadata":{"id":"WQPtK1QYVaD_","colab":{"base_uri":"https://localhost:8080/","height":65,"referenced_widgets":["8815626359d84416a2f44a95500580a4","3b85609c4ce94a74823f2cfe141ce68e","876609753c2946248890344722963d44","8abfdd8778e44b7ca0d29881cb1ada05","78c6c3d97c484916b8ee167c63556800","9dd0f182db5d45378ceafb855e486eb8","a3dab28b45c247089a3d1b8b09f327de","32451332b7a94ba9aacddeaa6ac94d50"]},"outputId":"81521192-cf67-4a47-a4cc-434cb0ebc363"}},{"cell_type":"code","execution_count":null,"source":["# Run YOLOv5x on COCO val2017\n","!python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65"],"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', exist_ok=False, img_size=640, iou_thres=0.65, name='exp', project='runs/test', save_conf=False, save_hybrid=False, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt'])\n","YOLOv5 ðŸš€ v5.0-1-g0f395b3 torch 1.8.1+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt to yolov5x.pt...\n","100% 168M/168M [00:05<00:00, 32.3MB/s]\n","\n","Fusing layers... \n","Model Summary: 476 layers, 87730285 parameters, 0 gradients, 218.8 GFLOPS\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '../coco/val2017' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 3102.29it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../coco/val2017.cache\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 157/157 [01:23<00:00,  1.87it/s]\n","                 all        5000       36335       0.745       0.627        0.68        0.49\n","Speed: 5.3/1.6/6.9 ms inference/NMS/total per 640x640 image at batch-size 32\n","\n","Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json...\n","loading annotations into memory...\n","Done (t=0.48s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=5.08s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=90.51s).\n","Accumulating evaluation results...\n","DONE (t=15.16s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n","Results saved to runs/test/exp\n"]}],"metadata":{"id":"X58w8JLpMnjH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2340b131-9943-4cd6-fd3a-8272aeb0774f"}},{"cell_type":"markdown","source":["## COCO test-dev2017\n","Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."],"metadata":{"id":"rc_KbFk0juX2"}},{"cell_type":"code","execution_count":null,"source":["# Download COCO test-dev2017\n","torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n","!unzip -q tmp.zip -d ../ && rm tmp.zip # unzip labels\n","!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images\n","%mv ./test2017 ../coco/images  # move to /coco"],"outputs":[],"metadata":{"id":"V0AJnSeCIHyJ"}},{"cell_type":"code","execution_count":null,"source":["# Run YOLOv5s on COCO test-dev2017 using --task test\n","!python test.py --weights yolov5s.pt --data coco.yaml --task test"],"outputs":[],"metadata":{"id":"29GJXAP_lPrt"}},{"cell_type":"markdown","source":["# 3. Train\n","\n","Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."],"metadata":{"id":"VUOiNLtMP5aG"}},{"cell_type":"code","execution_count":null,"source":["# Download COCO128\n","torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n","!unzip -q tmp.zip -d ../ && rm tmp.zip"],"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(FloatProgress(value=0.0, max=22091032.0), HTML(value='')))"],"application/vnd.jupyter.widget-view+json":{"model_id":"0fffa335322b41658508e06aed0acbf0","version_minor":0,"version_major":2}},"metadata":{"tags":[]}},{"output_type":"stream","name":"stdout","text":["\n"]}],"metadata":{"id":"Knxi2ncxWffW","colab":{"base_uri":"https://localhost:8080/","height":65,"referenced_widgets":["0fffa335322b41658508e06aed0acbf0","a354c6f80ce347e5a3ef64af87c0eccb","85823e71fea54c39bd11e2e972348836","fb11acd663fa4e71b041d67310d045fd","8a919053b780449aae5523658ad611fa","5bae9393a58b44f7b69fb04816f94f6f","d26c6d16c7f24030ab2da5285bf198ee","f7767886b2364c8d9efdc79e175ad8eb"]},"outputId":"b41ac253-9e1b-4c26-d78b-700ea0154f43"}},{"cell_type":"markdown","source":["Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n","\n","All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"],"metadata":{"id":"_pOkGLv1dMqh"}},{"cell_type":"code","execution_count":null,"source":["# Tensorboard (optional)\n","%load_ext tensorboard\n","%tensorboard --logdir runs/train"],"outputs":[],"metadata":{"id":"bOy5KI2ncnWd"}},{"cell_type":"code","execution_count":null,"source":["# Weights & Biases (optional)\n","%pip install -q wandb  \n","!wandb login  # use 'wandb disabled' or 'wandb enabled' to disable or enable"],"outputs":[],"metadata":{"id":"2fLAV42oNb7M"}},{"cell_type":"code","execution_count":null,"source":["# Train YOLOv5s on COCO128 for 3 epochs\n","!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache"],"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ðŸš€ v5.0-2-g54d6516 torch 1.8.1+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n","\n","Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='./data/coco128.yaml', device='', entity=None, epochs=3, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=True, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","2021-04-12 10:29:58.539457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n","  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPS\n","\n","Transferred 362/362 items from yolov5s.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../coco128/labels/train2017.cache' images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 796544.38it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB): 100% 128/128 [00:00<00:00, 176.73it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '../coco128/labels/train2017.cache' images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 500812.42it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB): 100% 128/128 [00:00<00:00, 134.10it/s]\n","Plotting labels... \n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.26, Best Possible Recall (BPR) = 0.9946\n","Image sizes 640 train, 640 test\n","Using 2 dataloader workers\n","Logging results to runs/train/exp\n","Starting training for 3 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       0/2     3.29G   0.04368     0.065   0.02127    0.1299       183       640: 100% 8/8 [00:03<00:00,  2.21it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:04<00:00,  1.09s/it]\n","                 all         128         929       0.605       0.657       0.666       0.434\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/2     6.65G   0.04556    0.0651   0.01987    0.1305       166       640: 100% 8/8 [00:01<00:00,  5.18it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:01<00:00,  2.72it/s]\n","                 all         128         929        0.61        0.66       0.669       0.438\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       2/2     6.65G   0.04624   0.06923    0.0196    0.1351       182       640: 100% 8/8 [00:01<00:00,  5.19it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:03<00:00,  1.27it/s]\n","                 all         128         929       0.618       0.659       0.671       0.438\n","3 epochs completed in 0.007 hours.\n","\n","Optimizer stripped from runs/train/exp/weights/last.pt, 14.8MB\n","Optimizer stripped from runs/train/exp/weights/best.pt, 14.8MB\n"]}],"metadata":{"id":"1NcFxRcFdJ_O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e715d09c-5d93-4912-a0df-9da0893f2014"}},{"cell_type":"markdown","source":["# 4. Visualize"],"metadata":{"id":"15glLzbQx5u0"}},{"cell_type":"markdown","source":["## Weights & Biases Logging ðŸŒŸ NEW\n","\n","[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n","\n","During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n","\n","<img src=\"https://user-images.githubusercontent.com/26833433/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"],"metadata":{"id":"DLI1JmHU7B0l"}},{"cell_type":"markdown","source":["## Local Logging\n","\n","All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."],"metadata":{"id":"-WPvRbS5Swl6"}},{"cell_type":"code","execution_count":null,"source":["Image(filename='runs/train/exp/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\n","Image(filename='runs/train/exp/test_batch0_labels.jpg', width=800)  # test batch 0 labels\n","Image(filename='runs/train/exp/test_batch0_pred.jpg', width=800)  # test batch 0 predictions"],"outputs":[],"metadata":{"id":"riPdhraOTCO0"}},{"cell_type":"markdown","source":["> <img src=\"https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg\" width=\"750\">  \n","`train_batch0.jpg` shows train batch 0 mosaics and labels\n","\n","> <img src=\"https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg\" width=\"750\">  \n","`test_batch0_labels.jpg` shows test batch 0 labels\n","\n","> <img src=\"https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg\" width=\"750\">  \n","`test_batch0_pred.jpg` shows test batch 0 _predictions_\n"],"metadata":{"id":"OYG4WFEnTVrI"}},{"cell_type":"markdown","source":["Training losses and performance metrics are also logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained `--weights yolov5s.pt` (orange)."],"metadata":{"id":"7KN5ghjE6ZWh"}},{"cell_type":"code","execution_count":null,"source":["from utils.plots import plot_results \n","plot_results(save_dir='runs/train/exp')  # plot all results*.txt as results.png\n","Image(filename='runs/train/exp/results.png', width=800)"],"outputs":[],"metadata":{"id":"MDznIqPF7nk3"}},{"cell_type":"markdown","source":["<img src=\"https://user-images.githubusercontent.com/26833433/97808309-8182b180-1c66-11eb-8461-bffe1a79511d.png\" width=\"800\">\n"],"metadata":{"id":"lfrEegCSW3fK"}},{"cell_type":"markdown","source":["# Environments\n","\n","YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n","\n","- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n","- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n","- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n","- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"],"metadata":{"id":"Zelyeqbyt3GD"}},{"cell_type":"markdown","source":["# Status\n","\n","![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n","\n","If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"],"metadata":{"id":"6Qu7Iesl0p54"}},{"cell_type":"markdown","source":["# Appendix\n","\n","Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"],"metadata":{"id":"IEijrePND_2I"}},{"cell_type":"code","execution_count":null,"source":["# Re-clone repo\n","%cd ..\n","%rm -rf yolov5 && git clone https://github.com/ultralytics/yolov5\n","%cd yolov5"],"outputs":[],"metadata":{"id":"gI6NoBev8Ib1"}},{"cell_type":"code","execution_count":null,"source":["# Reproduce\n","for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n","  !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n","  !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"],"outputs":[],"metadata":{"id":"mcKoSIK2WSzj"}},{"cell_type":"code","execution_count":null,"source":["# PyTorch Hub\n","import torch\n","\n","# Model\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n","\n","# Images\n","dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n","imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n","\n","# Inference\n","results = model(imgs)\n","results.print()  # or .show(), .save()"],"outputs":[],"metadata":{"id":"GMusP4OAxFu6"}},{"cell_type":"code","execution_count":null,"source":["# Unit tests\n","%%shell\n","export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n","\n","rm -rf runs  # remove runs/\n","for m in yolov5s; do  # models\n","  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n","  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n","  for d in 0 cpu; do  # devices\n","    python detect.py --weights $m.pt --device $d  # detect official\n","    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n","    python test.py --weights $m.pt --device $d # test official\n","    python test.py --weights runs/train/exp/weights/best.pt --device $d # test custom\n","  done\n","  python hubconf.py  # hub\n","  python models/yolo.py --cfg $m.yaml  # inspect\n","  python models/export.py --weights $m.pt --img 640 --batch 1  # export\n","done"],"outputs":[],"metadata":{"id":"FGH0ZjkGjejy"}},{"cell_type":"code","execution_count":null,"source":["# Profile\n","from utils.torch_utils import profile \n","\n","m1 = lambda x: x * torch.sigmoid(x)\n","m2 = torch.nn.SiLU()\n","profile(x=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"],"outputs":[],"metadata":{"id":"gogI-kwi3Tye"}},{"cell_type":"code","execution_count":null,"source":["# Evolve\n","!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n","!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"],"outputs":[],"metadata":{"id":"RVRSOhEvUdb5"}},{"cell_type":"code","execution_count":null,"source":["# VOC\n","for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n","  !python train.py --batch {b} --weights {m}.pt --data voc.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"],"outputs":[],"metadata":{"id":"BSgFCAcMbk1R"}}]}